{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Training Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('genre_classification_289a/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from model import STN, MLP\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from features import get_data_loaders, FramedFeatureDataset, FeatureDataset, DatasetSettings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP target always genre\n",
    "agfs = [] #'subgenre', 'mfcc'\n",
    "genre = True #False if not genre STN\n",
    "    \n",
    "#dataset\n",
    "dataset_name = 'fma_medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num genres:  16\n",
      "Rock                   6911\n",
      "Electronic             6110\n",
      "Experimental           2207\n",
      "Hip-Hop                2109\n",
      "Folk                   1477\n",
      "Instrumental           1280\n",
      "Pop                    1129\n",
      "International          1004\n",
      "Classical               598\n",
      "Old-Time / Historic     510\n",
      "Jazz                    380\n",
      "Country                 178\n",
      "Soul-RnB                154\n",
      "Spoken                  118\n",
      "Blues                    72\n",
      "Easy Listening           21\n",
      "Name: genre_top, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "settings = DatasetSettings(dataset_name, 'fma_metadata')\n",
    "dataset = FramedFeatureDataset(settings,  agfs=agfs, genre=genre)\n",
    "print(\"Num genres: \", settings.num_genres)\n",
    "print(settings.genre_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stn_path(dataset, target):\n",
    "    return '../models/DCNN_{}_{}'.format(dataset, target)\n",
    "\n",
    "# load STNs\n",
    "targets = ['subgenres']#, 'mfcc', 'genre']\n",
    "stns = [torch.load(get_stn_path(dataset_name, target)).to(device) for target in targets]\n",
    "stn_layer_dims = [None, 16, 32, 64, 64, 128, 256, 256]\n",
    "\n",
    "#which layer to extract features from\n",
    "layer = 7\n",
    "\n",
    "# setup MLP on GPU\n",
    "mlp_input_size = len(targets) * stn_layer_dims[layer]\n",
    "mlp_output_size = settings.num_genres\n",
    "mlp = MLP(mlp_input_size, mlp_output_size)\n",
    "mlp.to(device)\n",
    "mlp = nn.DataParallel(mlp)\n",
    "\n",
    "## Training Parameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "valid_split = 0.2\n",
    "\n",
    "trainloader, validloader = get_data_loaders(dataset, batch_size, valid_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def validate():\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for stn in stns:\n",
    "            stn.eval()\n",
    "        mlp.eval()\n",
    "                \n",
    "        all_pred = []\n",
    "        all_true = []\n",
    "        \n",
    "        for i, data in enumerate(validloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1]['genre'].to(device)\n",
    "            \n",
    "            out_intermediate = [stn.module.forward_intermediate(inputs, layer) for stn in stns]\n",
    "            input_mlp = torch.cat(out_intermediate, dim=1)\n",
    "            \n",
    "            out = mlp(input_mlp)\n",
    "            loss = F.cross_entropy(out, labels)\n",
    "            \n",
    "            all_pred.append(out.argmax(dim=1))\n",
    "            all_true.append(labels)\n",
    "            \n",
    "        all_pred = torch.cat(all_pred)\n",
    "        all_true = torch.cat(all_true)\n",
    "        \n",
    "        curr_f1 = f1_score(all_true.cpu(), all_pred.cpu(), average='micro')\n",
    "        return curr_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.72 Âµs\n",
      "Starting epoch 1\n",
      "[1,    30] loss: 1.709\n",
      "[1,    60] loss: 1.073\n",
      "[1,    90] loss: 0.868\n",
      "[1,   120] loss: 0.804\n",
      "[1,   150] loss: 0.779\n",
      "[1,   180] loss: 0.721\n",
      "[1,   210] loss: 0.712\n",
      "[1,   240] loss: 0.698\n",
      "[1,   270] loss: 0.644\n",
      "[1,   300] loss: 0.682\n",
      "[1,   330] loss: 0.656\n",
      "[1,   360] loss: 0.616\n",
      "[1,   390] loss: 0.593\n",
      "[1,   420] loss: 0.637\n",
      "[1,   450] loss: 0.648\n",
      "[1,   480] loss: 0.587\n",
      "[1,   510] loss: 0.595\n",
      "[1,   540] loss: 0.630\n",
      "[1,   570] loss: 0.592\n",
      "[1,   600] loss: 0.604\n",
      "[1,   630] loss: 0.664\n",
      "[1,   660] loss: 0.589\n",
      "[1,   690] loss: 0.653\n",
      "[1,   720] loss: 0.586\n",
      "[1,   750] loss: 0.545\n",
      "[1,   780] loss: 0.593\n",
      "[1,   810] loss: 0.648\n",
      "[1,   840] loss: 0.559\n",
      "[1,   870] loss: 0.555\n",
      "[1,   900] loss: 0.563\n",
      "[1,   930] loss: 0.583\n",
      "[1,   960] loss: 0.579\n",
      "[1,   990] loss: 0.591\n",
      "[1,  1020] loss: 0.601\n",
      "[1,  1050] loss: 0.640\n",
      "[1,  1080] loss: 0.546\n",
      "[1,  1110] loss: 0.522\n",
      "[1,  1140] loss: 0.599\n",
      "[1,  1170] loss: 0.562\n",
      "[1,  1200] loss: 0.557\n",
      "[1,  1230] loss: 0.530\n",
      "[1,  1260] loss: 0.558\n",
      "[1,  1290] loss: 0.590\n",
      "[1,  1320] loss: 0.603\n",
      "[1,  1350] loss: 0.574\n",
      "[1,  1380] loss: 0.571\n",
      "[1,  1410] loss: 0.564\n",
      "[1,  1440] loss: 0.543\n",
      "[1,  1470] loss: 0.609\n",
      "[1,  1500] loss: 0.548\n",
      "[1,  1530] loss: 0.599\n",
      "[1,  1560] loss: 0.596\n",
      "[1,  1590] loss: 0.572\n",
      "[1,  1620] loss: 0.567\n",
      "[1,  1650] loss: 0.616\n",
      "[1,  1680] loss: 0.538\n",
      "[1,  1710] loss: 0.589\n",
      "[1,  1740] loss: 0.575\n",
      "[1,  1770] loss: 0.565\n",
      "[1,  1800] loss: 0.527\n",
      "[1,  1830] loss: 0.533\n",
      "[1,  1860] loss: 0.644\n",
      "[1,  1890] loss: 0.576\n",
      "[1,  1920] loss: 0.588\n",
      "[1,  1950] loss: 0.580\n",
      "[1,  1980] loss: 0.608\n",
      "[1,  2010] loss: 0.560\n",
      "[1,  2040] loss: 0.593\n",
      "[1,  2070] loss: 0.542\n",
      "[1,  2100] loss: 0.533\n",
      "Starting epoch 2\n",
      "[2,    30] loss: 0.547\n",
      "[2,    60] loss: 0.555\n",
      "[2,    90] loss: 0.521\n",
      "[2,   120] loss: 0.525\n",
      "[2,   150] loss: 0.585\n",
      "[2,   180] loss: 0.553\n",
      "[2,   210] loss: 0.583\n",
      "[2,   240] loss: 0.521\n",
      "[2,   270] loss: 0.525\n",
      "[2,   300] loss: 0.565\n",
      "[2,   330] loss: 0.531\n",
      "[2,   360] loss: 0.604\n",
      "[2,   390] loss: 0.585\n",
      "[2,   420] loss: 0.580\n",
      "[2,   450] loss: 0.550\n",
      "[2,   480] loss: 0.551\n",
      "[2,   510] loss: 0.550\n",
      "[2,   540] loss: 0.551\n",
      "[2,   570] loss: 0.520\n",
      "[2,   600] loss: 0.546\n",
      "[2,   630] loss: 0.502\n",
      "[2,   660] loss: 0.561\n",
      "[2,   690] loss: 0.592\n",
      "[2,   720] loss: 0.553\n",
      "[2,   750] loss: 0.586\n",
      "[2,   780] loss: 0.536\n",
      "[2,   810] loss: 0.560\n",
      "[2,   840] loss: 0.536\n",
      "[2,   870] loss: 0.565\n",
      "[2,   900] loss: 0.556\n",
      "[2,   930] loss: 0.502\n",
      "[2,   960] loss: 0.547\n",
      "[2,   990] loss: 0.512\n",
      "[2,  1020] loss: 0.516\n",
      "[2,  1050] loss: 0.509\n",
      "[2,  1080] loss: 0.557\n",
      "[2,  1110] loss: 0.536\n",
      "[2,  1140] loss: 0.532\n",
      "[2,  1170] loss: 0.522\n",
      "[4,  1380] loss: 0.508\n",
      "[4,  1410] loss: 0.584\n",
      "[4,  1440] loss: 0.508\n",
      "[4,  1470] loss: 0.462\n",
      "[4,  1500] loss: 0.458\n",
      "[4,  1530] loss: 0.504\n",
      "[4,  1560] loss: 0.526\n",
      "[4,  1590] loss: 0.535\n",
      "[4,  1620] loss: 0.480\n",
      "[4,  1650] loss: 0.478\n",
      "[4,  1680] loss: 0.535\n",
      "[4,  1710] loss: 0.504\n",
      "[4,  1740] loss: 0.543\n",
      "[4,  1770] loss: 0.525\n",
      "[4,  1800] loss: 0.559\n",
      "[4,  1830] loss: 0.535\n",
      "[4,  1860] loss: 0.482\n",
      "[4,  1890] loss: 0.490\n",
      "[4,  1920] loss: 0.505\n",
      "[4,  1950] loss: 0.461\n",
      "[4,  1980] loss: 0.510\n",
      "[4,  2010] loss: 0.559\n",
      "[4,  2040] loss: 0.541\n",
      "[4,  2070] loss: 0.541\n",
      "[4,  2100] loss: 0.497\n",
      "Starting epoch 5\n",
      "[5,    30] loss: 0.479\n",
      "[5,    60] loss: 0.466\n",
      "[5,    90] loss: 0.506\n",
      "[5,   120] loss: 0.488\n",
      "[5,   150] loss: 0.535\n",
      "[5,   180] loss: 0.514\n",
      "[5,   210] loss: 0.493\n",
      "[5,   240] loss: 0.515\n",
      "[5,   270] loss: 0.545\n",
      "[5,   300] loss: 0.519\n",
      "[5,   330] loss: 0.507\n",
      "[5,   360] loss: 0.565\n",
      "[5,   390] loss: 0.517\n",
      "[5,   420] loss: 0.510\n",
      "[5,   450] loss: 0.531\n",
      "[5,   480] loss: 0.587\n",
      "[5,   510] loss: 0.581\n",
      "[5,   900] loss: 0.478\n",
      "[5,   930] loss: 0.463\n",
      "[5,   960] loss: 0.491\n",
      "[5,   990] loss: 0.506\n",
      "[5,  1020] loss: 0.475\n",
      "[5,  1050] loss: 0.526\n",
      "[5,  1080] loss: 0.442\n",
      "[5,  1110] loss: 0.512\n",
      "[5,  1140] loss: 0.527\n",
      "[5,  1170] loss: 0.527\n",
      "[5,  1200] loss: 0.491\n",
      "[5,  1230] loss: 0.484\n",
      "[5,  1260] loss: 0.509\n",
      "[5,  1290] loss: 0.454\n",
      "[5,  1320] loss: 0.436\n",
      "[5,  1350] loss: 0.494\n",
      "[5,  1380] loss: 0.540\n",
      "[5,  1410] loss: 0.555\n",
      "[5,  1440] loss: 0.501\n",
      "[5,  1470] loss: 0.509\n",
      "[5,  1500] loss: 0.521\n",
      "[5,  1530] loss: 0.527\n",
      "[5,  1560] loss: 0.521\n",
      "[5,  1590] loss: 0.496\n",
      "[5,  1620] loss: 0.488\n",
      "[5,  1650] loss: 0.465\n",
      "[5,  1680] loss: 0.475\n",
      "[5,  1710] loss: 0.515\n",
      "[5,  1740] loss: 0.497\n",
      "[5,  1770] loss: 0.502\n",
      "[5,  1800] loss: 0.456\n",
      "[5,  1830] loss: 0.533\n",
      "[5,  1860] loss: 0.501\n",
      "[5,  1890] loss: 0.499\n",
      "[5,  1920] loss: 0.503\n",
      "[5,  1950] loss: 0.493\n",
      "[5,  1980] loss: 0.489\n",
      "[5,  2010] loss: 0.531\n",
      "[5,  2040] loss: 0.514\n",
      "[5,  2070] loss: 0.518\n",
      "[5,  2100] loss: 0.513\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Train it\n",
    "%time\n",
    "losses = []\n",
    "accs = []\n",
    "for stn in stns:\n",
    "    stn.eval()\n",
    "\n",
    "\n",
    "#f.write('Initial Validation F1: %.6f' % validate())\n",
    "\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    print('Starting epoch %d' % (epoch+1))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data[0].to(device), data[1]['genre'].to(device)  #data[1]['{argument for agf being trained}']\n",
    "        \n",
    "        input_mlp = None\n",
    "        with torch.no_grad():\n",
    "            out_intermediates = [stn.module.forward_intermediate(inputs, layer) for stn in stns]\n",
    "            input_mlp = torch.cat(out_intermediates, dim=1)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = mlp(input_mlp)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 30 == 29:    # print every 30 mini-batches\n",
    "            avg_loss = running_loss / 30\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, avg_loss))\n",
    "            losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "final_f1 = validate()\n",
    "np.array(losses).tofile(f'logs/losses_MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layer}')\n",
    "np.array(final_f1).tofile(f'logs/final_MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = f'../models/MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layer}'\n",
    "torch.save(mlp, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Eval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = f'../models/MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layer}'\n",
    "mlp = torch.load(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load losses and final accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.fromfile(f'logs/losses_MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layer}')\n",
    "final_f1 = np.fromfile(f'logs/final_MLP_{dataset_name}_stn_{\"_\".join(targets)}_layer_{layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86016313])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Performance History\n",
    "* SGM layer 4: 0.67209446\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
